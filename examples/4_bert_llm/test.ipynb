{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed70a480-3686-4522-acee-ae48079579d2",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">\n",
    "  <a href=\"https://uptrain.ai\">\n",
    "    <img width=\"300\" src=\"https://user-images.githubusercontent.com/108270398/214240695-4f958b76-c993-4ddd-8de6-8668f4d0da84.png\" alt=\"uptrain\">\n",
    "  </a>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074abda-7ad0-4af1-a7b4-f5177213dae8",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Fine-tuning a Large-Language Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f34304-dc81-4fd9-94dc-a7dfc371673a",
   "metadata": {},
   "source": [
    "### Install Required packages\n",
    "- [PyTorch](https://pytorch.org/get-started/locally/): Deep learning framework.\n",
    "- Hugging Face Transformers(https://huggingface.co/docs/transformers/installation): To use pretrained state-of-the-art models.\n",
    "- [Hugging Face Datasets](https://pypi.org/project/datasets/): Use public Hugging Face datasets\n",
    "- [IPywidgets](https://ipywidgets.readthedocs.io/en/stable/user_install.html): For interactive notebook widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61005f8-27ae-4a7a-a190-fe5f335821cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from model_constants import *\n",
    "from model_train import retrain_model\n",
    "from helper_funcs import *\n",
    "import json\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import uptrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d6e1263-e704-4de2-af99-507ea68125fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "testing_text = \"Nike shoes are very [MASK].\"\n",
    "original_model_outputs = test_model(model, testing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       n_reviews  score comfort durability  \\\n0          169.0    4.6    90.0       76.0   \n1           62.0    4.3    81.0       50.0   \n2           36.0    4.0    87.5       93.0   \n3          125.0    4.2    79.5       75.5   \n4           51.0    3.7    70.5       61.0   \n...          ...    ...     ...        ...   \n23273       72.0    4.8    85.5       68.0   \n23274       72.0    4.8    85.5       68.0   \n23275       72.0    4.8    85.5       68.0   \n23276       72.0    4.8    85.5       68.0   \n23277      943.0    4.8    94.0       76.0   \n\n                                                 r_title  r_raiting  \\\n0                                    Not worth the money        1.0   \n1                                           Disappointed        1.0   \n2                                                    NaN        1.0   \n3      very uncomfortable wearing and doesn't look good.        1.0   \n4                                    ripped on first use        1.0   \n...                                                  ...        ...   \n23273                                        Jordan 92's        5.0   \n23274                           Awesomely fast shipping.        5.0   \n23275                                                NaN        5.0   \n23276                                   Highly recommend        5.0   \n23277                                       Amazing shoe        5.0   \n\n                                                  r_body  \n0      Choose another shoe. Not worth it. Size too sm...  \n1      Nice look but eyelets broke in less than month...  \n2      First pair of Nike shoes that didn’t come clos...  \n3      I bought these pair of nike shoes recently, on...  \n4      The fabric at the toe ripped while skating the...  \n...                                                  ...  \n23273  Bought the shoes as a birthday gift and they l...  \n23274  Super fast shipping. Make sure you make a Nike...  \n23275                                         Nice shoe.  \n23276                                  Shoes feel great!  \n23277  I have thought about getting this shoe for a w...  \n\n[23278 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_reviews</th>\n      <th>score</th>\n      <th>comfort</th>\n      <th>durability</th>\n      <th>r_title</th>\n      <th>r_raiting</th>\n      <th>r_body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>169.0</td>\n      <td>4.6</td>\n      <td>90.0</td>\n      <td>76.0</td>\n      <td>Not worth the money</td>\n      <td>1.0</td>\n      <td>Choose another shoe. Not worth it. Size too sm...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>62.0</td>\n      <td>4.3</td>\n      <td>81.0</td>\n      <td>50.0</td>\n      <td>Disappointed</td>\n      <td>1.0</td>\n      <td>Nice look but eyelets broke in less than month...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36.0</td>\n      <td>4.0</td>\n      <td>87.5</td>\n      <td>93.0</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>First pair of Nike shoes that didn’t come clos...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>125.0</td>\n      <td>4.2</td>\n      <td>79.5</td>\n      <td>75.5</td>\n      <td>very uncomfortable wearing and doesn't look good.</td>\n      <td>1.0</td>\n      <td>I bought these pair of nike shoes recently, on...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>51.0</td>\n      <td>3.7</td>\n      <td>70.5</td>\n      <td>61.0</td>\n      <td>ripped on first use</td>\n      <td>1.0</td>\n      <td>The fabric at the toe ripped while skating the...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23273</th>\n      <td>72.0</td>\n      <td>4.8</td>\n      <td>85.5</td>\n      <td>68.0</td>\n      <td>Jordan 92's</td>\n      <td>5.0</td>\n      <td>Bought the shoes as a birthday gift and they l...</td>\n    </tr>\n    <tr>\n      <th>23274</th>\n      <td>72.0</td>\n      <td>4.8</td>\n      <td>85.5</td>\n      <td>68.0</td>\n      <td>Awesomely fast shipping.</td>\n      <td>5.0</td>\n      <td>Super fast shipping. Make sure you make a Nike...</td>\n    </tr>\n    <tr>\n      <th>23275</th>\n      <td>72.0</td>\n      <td>4.8</td>\n      <td>85.5</td>\n      <td>68.0</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>Nice shoe.</td>\n    </tr>\n    <tr>\n      <th>23276</th>\n      <td>72.0</td>\n      <td>4.8</td>\n      <td>85.5</td>\n      <td>68.0</td>\n      <td>Highly recommend</td>\n      <td>5.0</td>\n      <td>Shoes feel great!</td>\n    </tr>\n    <tr>\n      <th>23277</th>\n      <td>943.0</td>\n      <td>4.8</td>\n      <td>94.0</td>\n      <td>76.0</td>\n      <td>Amazing shoe</td>\n      <td>5.0</td>\n      <td>I have thought about getting this shoe for a w...</td>\n    </tr>\n  </tbody>\n</table>\n<p>23278 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd1 = pd.read_csv('raw_data/training_data.csv')\n",
    "pd1.drop(columns=['id_','gender','title','url','category','price','description','description_long','size','r_date'],inplace=True)\n",
    "pd1.drop(pd1[pd1['r_body'].isna() & pd1['r_title'].isna()].index,inplace=True)\n",
    "pd1.sort_values(by='r_raiting',ascending=True, inplace=True, kind='quicksort')\n",
    "pd1.reset_index(inplace=True,drop=True)\n",
    "pd1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "real_world_test_cases = 'raw_data/real_world_testing_data.csv'\n",
    "training_dataset = create_dataset_from_csv(real_world_test_cases, \"r_title\", \"real_world_testing_data.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9edc5da-8366-4c55-93a7-4601c89c9970",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  uptrain_smart_data_bert\n"
     ]
    }
   ],
   "source": [
    "def is_negative_sentiment(inputs, outputs, gts=None, extra_args={}):\n",
    "    is_negatives = []\n",
    "    for txt in inputs[\"text\"]:\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        score = sia.polarity_scores(txt.lower())\n",
    "        is_negative = score['pos'] < 0.25\n",
    "        if score['compound'] <= - 0.05:\n",
    "            is_negative = True\n",
    "        is_negatives.append(bool(is_negative))\n",
    "    return is_negatives\n",
    "\n",
    "uptrain_save_fold_name = \"uptrain_smart_data_bert\"\n",
    "nike_text_present = uptrain.Signal(\"Nike Text Present\", is_negative_sentiment)\n",
    "\n",
    "cfg = {\n",
    "    'checks': [{\n",
    "        'type': uptrain.Anomaly.EDGE_CASE,\n",
    "        \"signal_formulae\": nike_text_present\n",
    "    }],\n",
    "\n",
    "    # Define where to save the retraining dataset\n",
    "    'retraining_folder': uptrain_save_fold_name,\n",
    "    \n",
    "    # Define when to retrain, define a large number because we\n",
    "    # are not retraining yet\n",
    "    'retrain_after': 100\n",
    "}\n",
    "\n",
    "framework = uptrain.Framework(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abe66970-c0ce-421d-9f7a-845453615903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  edge cases identified out of  142  total samples\n",
      "100  edge cases identified out of  263  total samples\n",
      "\n",
      "Kicking off re-training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetHandler' object has no attribute 'annotation_method'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m all_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[0;32m      6\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m: [sample[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]]}}\n\u001B[1;32m----> 7\u001B[0m     \u001B[43mframework\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m retraining_dataset \u001B[38;5;241m=\u001B[39m create_dataset_from_csv(uptrain_save_fold_name \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/1/smart_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrain_dataset.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\uptrain-ai\\venv\\lib\\site-packages\\uptrain\\core\\classes\\framework.py:385\u001B[0m, in \u001B[0;36mFramework.log\u001B[1;34m(self, inputs, outputs, gts, identifiers, extra)\u001B[0m\n\u001B[0;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattach_ground_truth({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m: identifiers, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgt\u001B[39m\u001B[38;5;124m\"\u001B[39m: gts})\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneed_retraining():\n\u001B[1;32m--> 385\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m identifiers\n",
      "File \u001B[1;32m~\\Documents\\uptrain-ai\\venv\\lib\\site-packages\\uptrain\\core\\classes\\framework.py:268\u001B[0m, in \u001B[0;36mFramework.retrain\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    265\u001B[0m retraining_data \u001B[38;5;241m=\u001B[39m df_smart_data\u001B[38;5;241m.\u001B[39mto_dict(orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecords\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    267\u001B[0m \u001B[38;5;66;03m# Generate training dataset\u001B[39;00m\n\u001B[1;32m--> 268\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_retraining_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretraining_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morig_training_file\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[38;5;66;03m# Retrain the model\u001B[39;00m\n\u001B[0;32m    272\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_handler\u001B[38;5;241m.\u001B[39mretrain(\n\u001B[0;32m    273\u001B[0m     dataset_location \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/training_dataset.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mversion\n\u001B[0;32m    274\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\uptrain-ai\\venv\\lib\\site-packages\\uptrain\\core\\classes\\helpers\\dataset_handler.py:82\u001B[0m, in \u001B[0;36mDatasetHandler.create_retraining_dataset\u001B[1;34m(self, dataset_location, new_data, old_dataset, ratio)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcluster_plot_func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     75\u001B[0m     cluster_and_plot_data(\n\u001B[0;32m     76\u001B[0m         np\u001B[38;5;241m.\u001B[39marray([x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkps\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m new_data]),\n\u001B[0;32m     77\u001B[0m         \u001B[38;5;241m9\u001B[39m,\n\u001B[0;32m     78\u001B[0m         cluster_plot_func\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcluster_plot_func,\n\u001B[0;32m     79\u001B[0m         plot_save_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcollected_edge_cases_clusters.png\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     80\u001B[0m     )\n\u001B[1;32m---> 82\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_annotations\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_location\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/cleaned_dataset.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     83\u001B[0m write_json(dataset_location \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/annotated_dataset.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_data)\n\u001B[0;32m     84\u001B[0m write_json(\n\u001B[0;32m     85\u001B[0m     dataset_location \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/training_dataset.json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmerge_training_datasets(\n\u001B[0;32m     87\u001B[0m         old_dataset, dataset_location \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/annotated_dataset.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, ratio\u001B[38;5;241m=\u001B[39mratio\n\u001B[0;32m     88\u001B[0m     ),\n\u001B[0;32m     89\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\uptrain-ai\\venv\\lib\\site-packages\\uptrain\\core\\classes\\helpers\\dataset_handler.py:37\u001B[0m, in \u001B[0;36mDatasetHandler.add_annotations\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_annotations\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset):\n\u001B[0;32m     35\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Adds gt to the dataset file by calling appropriate funcs on the annotation helper based on attached annotation method\"\"\"\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mannotation_method\u001B[49m \u001B[38;5;241m==\u001B[39m AnnotationMethod\u001B[38;5;241m.\u001B[39mMASTER_FILE:\n\u001B[0;32m     38\u001B[0m         annotated_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mannotation_helper\u001B[38;5;241m.\u001B[39mannotations_from_master_file(\n\u001B[0;32m     39\u001B[0m             dataset\n\u001B[0;32m     40\u001B[0m         )\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DatasetHandler' object has no attribute 'annotation_method'"
     ]
    }
   ],
   "source": [
    "# raw_dataset = create_sample_dataset(\"data.json\")\n",
    "with open('real_world_testing_data.json','r') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "for sample in all_data['data']:\n",
    "    inputs = {'data': {'text': [sample['text']]}}\n",
    "    framework.log(inputs = inputs, outputs = None)\n",
    "\n",
    "retraining_dataset = create_dataset_from_csv(uptrain_save_fold_name + \"/1/smart_data.csv\", \"text\", \"retrain_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8d91d9-4540-4850-ad27-69fe9468e7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2370e3cf0f5387dd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Downloading and preparing dataset json/default to /Users/vipul/.cache/huggingface/datasets/json/default-2370e3cf0f5387dd/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3febc7766304c9b84a1f06ae414fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66dff810d5647bfa1848aafa2ae36c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46565a9a9244205b9a01af3de48b50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/vipul/.cache/huggingface/datasets/json/default-2370e3cf0f5387dd/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9d507379f947adb8b5553b0e29592c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33d8af89d6b48fba1382d275d7b6975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05defffbcbc4cd0b17d3f0212e7836d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 02:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/vipul/opt/anaconda3/envs/uptrain_test/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 92\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "  Number of trainable parameters = 66985530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Before training, Perplexity: 10.48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 07:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.631900</td>\n",
       "      <td>0.936632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.283500</td>\n",
       "      <td>1.153624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.151900</td>\n",
       "      <td>0.998965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 11\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>After trainign, Perplexity: 2.63\n"
     ]
    }
   ],
   "source": [
    "retrain_model(model, retraining_dataset)\n",
    "retrained_model_outputs = test_model(model, testing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f014895c-0e6d-4886-a9b4-7071154e6315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print([original_model_outputs, retrained_model_outputs])\n",
    "\n",
    "# # Create Nike review training dataset\n",
    "# nike_attrs = {\n",
    "#     \"version\": \"0.1.0\",\n",
    "#     'source': \"nike review dataset\",\n",
    "#     'url': 'https://www.kaggle.com/datasets/tinkuzp23/nike-onlinestore-customer-reviews?resource=download',\n",
    "# }\n",
    "# # Download the dataset from the url, zip it and copy the csv file here\n",
    "# raw_nike_reviews_dataset = create_dataset_from_csv(\"web_scrapped.csv\", \"Content\", \"raw_nike_reviews_data.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
